---
title: "Kaggle: Predicting Housing Prices"
author: "Lizhizi Cui (lc3268) and Mario Saraiva (mhs2195)"
date: "3/28/2018"
output: 
    html_document:
    code_folding: hide
    number_sections: yes
    toc: yes
    toc_float: yes
---

<center>
`Applied Data Science for Social Sciences Project: Preliminary results`
  
`Lizhizi Cui (lc3268) and Mario Saraiva (mhs2195)`
  
`March 28, 2018`
</center>

<p>
  
  
</p> 

```{r}
knitr::opts_chunk$set(echo = TRUE)
```

#Introduction

#### Overview: We want to accurately predict Sales Price based on other variables in the dataset and we will use our findings from inferential section to create a more robust model. 

#### Data: We used a dataset from Kaggle which contain information of 1460 houses in Ames, Iowa. There are 81 variables in total, of which 37 are quantitative including the final Sales Price of the house, 43 are categorical + the Id number of each house.


| Variable      | Quantitative? | Class  |
|---------------|---------------|--------|
| 1stFlrSF      | Yes           | int    |
| 2ndFlrSF      | Yes           | int    |
| 3SsnPorch     | Yes           | int    |
| BedroomAbvGr  | Yes           | int    |
| BsmtFinSF1    | Yes           | int    |
| BsmtFinSF2    | Yes           | int    |
| BsmtFullBath  | Yes           | int    |
| BsmtHalfBath  | Yes           | int    |
| BsmtUnfSF     | Yes           | int    |
| EnclosedPorch | Yes           | int    |
| Fireplaces    | Yes           | int    |
| FullBath      | Yes           | int    |
| GarageArea    | Yes           | int    |
| GarageCars    | Yes           | int    |
| GarageYrBlt   | Yes           | int    |
| GrLivArea     | Yes           | int    |
| HalfBath      | Yes           | int    |
| KitchenAbvGr  | Yes           | int    |
| LotArea       | Yes           | int    |
| LotFrontage   | Yes           | int    |
| LowQualFinSF  | Yes           | int    |
| MasVnrArea    | Yes           | int    |
| MiscVal       | Yes           | int    |
| MoSold        | Yes           | int    |
| MSSubClass    | Yes           | int    |
| OpenPorchSF   | Yes           | int    |
| OverallCond   | Yes           | int    |
| OverallQual   | Yes           | int    |
| PoolArea      | Yes           | int    |
| ScreenPorch   | Yes           | int    |
| TotalBsmtSF   | Yes           | int    |
| TotRmsAbvGrd  | Yes           | int    |
| WoodDeckSF    | Yes           | int    |
| YearBuilt     | Yes           | int    |
| YearRemodAdd  | Yes           | int    |
| YrSold.       | Yes           | int    |
| Alley         | No            | Factor |
| BldgType      | No            | Factor |
| BsmtCond      | No            | Factor |
| BsmtExposure  | No            | Factor |
| BsmtFinType1  | No            | Factor |
| BsmtFinType2  | No            | Factor |
| BsmtQual      | No            | Factor |
| CentralAir    | No            | Factor |
| Condition1    | No            | Factor |
| Condition2    | No            | Factor |
| Electrical    | No            | Factor |
| ExterCond     | No            | Factor |
| Exterior1st   | No            | Factor |
| Exterior2nd   | No            | Factor |
| ExterQual     | No            | Factor |
| Fence         | No            | Factor |
| FireplaceQu   | No            | Factor |
| Foundation    | No            | Factor |
| Functional    | No            | Factor |
| GarageCond    | No            | Factor |
| GarageFinish  | No            | Factor |
| GarageQual    | No            | Factor |
| GarageType    | No            | Factor |
| Heating       | No            | Factor |
| HeatingQC     | No            | Factor |
| HouseStyle    | No            | Factor |
| KitchenQual   | No            | Factor |
| LandContour   | No            | Factor |
| LandSlope     | No            | Factor |
| LotConfig     | No            | Factor |
| LotShape      | No            | Factor |
| MasVnrType    | No            | Factor |
| MiscFeature   | No            | Factor |
| MSZoning      | No            | Factor |
| Neighborhood  | No            | Factor |
| PavedDrive    | No            | Factor |
| PoolQC        | No            | Factor |
| RoofMatl      | No            | Factor |
| RoofStyle     | No            | Factor |
| SaleCondition | No            | Factor |
| SaleType      | No            | Factor |
| Street        | No            | Factor |
| Utilities.    | No            | Factor |

`Quantitative`: 1stFlrSF, 2ndFlrSF, 3SsnPorch, BedroomAbvGr, BsmtFinSF1, BsmtFinSF2, BsmtFullBath, BsmtHalfBath, BsmtUnfSF, EnclosedPorch, Fireplaces, FullBath, GarageArea, GarageCars, GarageYrBlt, GrLivArea, HalfBath, KitchenAbvGr, LotArea, LotFrontage, LowQualFinSF, MSSubClass, MasVnrArea, MiscVal, MoSold, OpenPorchSF, OverallCond, OverallQual, PoolArea, ScreenPorch, TotRmsAbvGrd, TotalBsmtSF, WoodDeckSF, YearBuilt, YearRemodAdd, YrSold.

`Qualitative`: Alley, BldgType, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, BsmtQual, CentralAir, Condition1, Condition2, Electrical, ExterCond, ExterQual, Exterior1st, Exterior2nd, Fence, FireplaceQu, Foundation, Functional, GarageCond, GarageFinish, GarageQual, GarageType, Heating, HeatingQC, HouseStyle, KitchenQual, LandContour, LandSlope, LotConfig, LotShape, MSZoning, MasVnrType, MiscFeature, Neighborhood, PavedDrive, PoolQC, RoofMatl, RoofStyle, SaleCondition, SaleType, Street, Utilities.

#### Struture: 
  1. Exploratary Data Analysis 
  2. Inference
  3. Predictive Modeling
  4. Conclusion

## 1. Exploratary Data Analysis 
```{r setup, include=FALSE}
library(glmnet)
library(purrr)
library(tidyr)
library(readr)
library(corrplot)
library(shiny)
library(dplyr)
library(ggplot2)
library(e1071)
library(glmnet)
library(mice)
library(lattice)
library(VIM)
library(caret)
library(stargazer)
library(gam)
library(flam)
library(bartMachine)
library(tree)
library(ISLR)
library(RColorBrewer)
library(ggfortify)
library(scales)
library(stargazer)
library(rsconnect)
library(knitr)
library(gridExtra)
library(scales)
library(Rmisc)
library(ggrepel)
library(randomForest)
library(psych)
library(purrr)
library(tidyr)
library(ggplot2)
library(readr)
library(dplyr)
library(corrplot)
library(RColorBrewer)
library(shiny)
library(ggfortify)
library(pander)
library(purrr)
library(tidyr)
library(ggplot2)
library(readr)
library(dplyr)
library(corrplot)
library(RColorBrewer)
library(shiny)
library(ggfortify)
library(pander)
library(here)
```

```{r}
all <- read.csv("/Users/lizhizicui/Desktop/Final-Project---Applied-Data-Science/Data/Raw//train.csv") # read the data

set.seed(12345)
in_train <- createDataPartition(y = all$SalePrice, p = 3 / 4, list = FALSE) # Split data into training and testing and recombine them
str(in_train)
training <- all[ in_train, ] # three fourth of data go to training
testing  <- all[-in_train, ]

all <- rbind(training, testing)
```

### 1.1 Explore missing values
 
```{r, warning=FALSE, message=FALSE}
na_count <-sapply(all, function(y) sum(length(which(is.na(y)))))
na_count <- data.frame(na_count)
```

Most missing: "PoolQC", "MiscFeature", "Alley", "Fence", "FireplaceQu", "LotFrontage"

Variable | Count of Missing
---------|-----------------
PoolQC | 1453
MiscFeature | 1406
Alley | 1369
Fence | 1179
FireplaceQu | 690
LotFrontage | 259
GarageType | 81
GarageYrBlt | 81
GarageFinish | 81
GarageQual81
GarageCond | 81
BsmtExposur | 38
BsmtFinType2 | 38
BsmtQual | 37
BsmtCond | 37
BsmtFinType1 | 37
MasVnrType | 8

PoolQC: Pool quality
		
       Ex	Excellent
       Gd	Good
       TA	Average/Typical
       Fa	Fair
       NA	No Pool
       
MiscFeature: Miscellaneous feature not covered in other categories
		
       Elev	Elevator
       Gar2	2nd Garage (if not described in garage section)
       Othr	Other
       Shed	Shed (over 100 SF)
       TenC	Tennis Court
       NA	None
       
Alley: Type of alley access to property

       Grvl	Gravel
       Pave	Paved
       NA 	No alley access     
       
Fence: Fence quality
		
       GdPrv	Good Privacy
       MnPrv	Minimum Privacy
       GdWo	Good Wood
       MnWw	Minimum Wood/Wire
       NA	No Fence     
       
FireplaceQu: Fireplace quality

       Ex	Excellent - Exceptional Masonry Fireplace
       Gd	Good - Masonry Fireplace in main level
       TA	Average - Prefabricated Fireplace in main living area or Masonry Fireplace in basement
       Fa	Fair - Prefabricated Fireplace in basement
       Po	Poor - Ben Franklin Stove
       NA	No Fireplace   
       
GarageType: Garage location (**smae for GarageYrBlt/GarageFinish/GarageCars/GarageArea/GarageQual/GarageCond)
		
       2Types	More than one type of garage
       Attchd	Attached to home
       Basment	Basement Garage
       BuiltIn	Built-In (Garage part of house - typically has room above garage)
       CarPort	Car Port
       Detchd	Detached from home
       NA	No Garage    
       
BsmtExposure: Refers to walkout or garden level walls (**smae for BsmtFinType1/BsmtFinSF1/BsmtFinType2/BsmtFinSF2/BsmtUnfSF/BsmtQual)

       Gd	Good Exposure
       Av	Average Exposure (split levels or foyers typically score average or above)	
       Mn	Mimimum Exposure
       No	No Exposure
       NA	No Basement

### 1.2 Dealing with NA
Created a new data file containing 1451 observations and 79 variables after removing missing values.
```{r, PoolQC}
all_test <- all
levels(all_test$PoolQC)

addLevel <- function(x, newlevel=NULL) {
  if(is.factor(x)) {
    if (is.na(match(newlevel, levels(x))))
      return(factor(x, levels=c(levels(x), newlevel)))
  }
  return(x)
}
all_test$PoolQC <- addLevel(all_test$PoolQC, "None")
all_test$PoolQC <- addLevel(all_test$PoolQC, "Po")
all_test$PoolQC <- addLevel(all_test$PoolQC, "TA")
all_test$PoolQC <- ifelse(is.na(all$PoolQC), "None", paste(all$PoolQC))

Qualities <- c('None' = 0, 'Po' = 1, 'Fa' = 2, 'TA' = 3, 'Gd' = 4, 'Ex' = 5)
all_test$PoolQC<- revalue(all_test$PoolQC, Qualities)
all_test$PoolQC <- as.factor(all_test$PoolQC)
```

```{r, MiscFeature}
all_test$MiscFeature <- addLevel(all_test$MiscFeature, "None")
all_test$MiscFeature <- ifelse(is.na(all_test$MiscFeature), "None", paste(all_test$MiscFeature))
all_test$MiscFeature <- as.factor(all_test$MiscFeature)
table(all_test$MiscFeature)
                                                                                            
all_test$Alley <- addLevel(all_test$Alley, "None")
all_test$Alley <- ifelse(is.na(all_test$Alley), "None", paste(all$Alley))
all_test$Alley <- as.factor(all_test$Alley)
table(all_test$Alley)

all_test$Fence <- addLevel(all_test$Fence, "None")
all_test$Fence <- ifelse(is.na(all$Fence), "None", paste(all_test$Fence))
all_test$Fence <- as.factor(all_test$Fence)
table(all_test$Fence)

all_test$FireplaceQu <- addLevel(all_test$FireplaceQu, "None")
all_test$FireplaceQu <- ifelse(!is.na(all_test$FireplaceQu), paste(all_test$FireplaceQu), "None")
all_test$FireplaceQu <- as.factor(all_test$FireplaceQu)


#GarageYrBlt/GarageFinish/GarageCars/GarageArea/GarageQual/GarageCond
all_test$GarageType <- addLevel(all_test$GarageType, "None")
all_test$GarageType <- ifelse(is.na(all_test$GarageType), "None", paste(all_test$GarageType))
all_test$GarageType <- as.factor(all_test$GarageType)
table(all_test$GarageType)


all_test$GarageYrBlt[is.na(all_test$GarageYrBlt)] <- 'No Garage'
all$GarageYrBlt <- as.numeric(all$GarageYrBlt)

all_test$GarageFinish <- addLevel(all_test$GarageFinish, "None")
all_test$GarageFinish <- ifelse(is.na(all_test$GarageFinish), "None", paste(all_test$GarageFinish))
all_test$GarageFinish <- as.factor(all_test$GarageFinish)
table(all_test$GarageFinish)

all_test$GarageCars[is.na(all_test$GarageCars)] <- 'No Garage'
all_test$GarageCars <- as.factor(all_test$GarageCars)
table(all_test$GarageCars)

all_test$GarageArea[is.na(all_test$GarageArea)] <- 'No Garage'
table(all_test$GarageArea)

all_test$GarageQual <- addLevel(all_test$GarageQual, "None")
all_test$GarageQual <- ifelse(is.na(all_test$GarageQual), "None", paste(all_test$GarageQual))
all_test$GarageQual <- as.factor(all_test$GarageQual)
table(all_test$GarageQual)

all_test$GarageCond <- addLevel(all_test$GarageCond, "None")
all_test$GarageCond <- ifelse(is.na(all_test$GarageCond), "None", paste(all_test$GarageCond))
all_test$GarageCond <- as.factor(all_test$GarageCond)
table(all_test$GarageCond)

#BsmtExposure: Refers to walkout or garden level walls (**smae for BsmtFinType1/BsmtFinSF1/BsmtFinType2/BsmtFinSF2/BsmtUnfSF/BsmtQual)
all_test$BsmtQual <- addLevel(all_test$BsmtQual, "None")
all_test$BsmtQual <- ifelse(is.na(all_test$BsmtQual), "None", paste(all_test$BsmtQual))
all_test$BsmtQual <- as.factor(all_test$BsmtQual)
table(all_test$BsmtQual)

all_test$BsmtExposure <- addLevel(all_test$BsmtExposure, "None")
all_test$BsmtExposure <- ifelse(is.na(all_test$BsmtExposure), "None", paste(all_test$BsmtExposure))
all_test$BsmtExposure <- as.factor(all_test$BsmtExposure)
table(all_test$BsmtExposure)

all_test$BsmtFinType1 <- addLevel(all_test$BsmtFinType1, "None")
all_test$BsmtFinType1 <- ifelse(is.na(all_test$BsmtFinType1), "None", paste(all_test$BsmtFinType1))
all_test$BsmtFinType1 <- as.factor(all_test$BsmtFinType1)
str(all_test$BsmtFinType1)

all_test$BsmtCond <- addLevel(all_test$BsmtCond, "None")
all_test$BsmtCond <- ifelse(is.na(all_test$BsmtCond), "None", paste(all_test$BsmtCond))
all_test$BsmtCond <- as.factor(all_test$BsmtCond)
table(all_test$BsmtCond)

all_test$BsmtFinType2 <- addLevel(all_test$BsmtFinType2, "None")
all_test$BsmtFinType2 <- ifelse(is.na(all_test$BsmtFinType2), "None", paste(all_test$BsmtFinType2))
all_test$BsmtFinType2 <- as.factor(all_test$BsmtFinType2)
table(all_test$BsmtFinType2)

all_test$BsmtFinSF1[is.na(all_test$BsmtFinSF1)] <-0
all_test$BsmtFinSF2[is.na(all_test$BsmtFinSF2)] <-0
all_test$BsmtUnfSF[is.na(all_test$BsmtUnfSF)] <-0
all_test$LotFrontage[is.na(all_test$LotFrontage)] <-0
table(all_test$LotFrontage)

drop.na.columns <- c( "MasVnrType","LotFrontage")
all_test<- all_test[ , !(names(all_test) %in% drop.na.columns)]

all_test <- na.omit(all_test)

```

```{r}
# factorize some of the variables
all <- all_test
all$Foundation <- as.factor(all$Foundation)
all$Heating <- as.factor(all$Heating)
all$RoofStyle <- as.factor(all$RoofStyle)
all$RoofMatl <- as.factor(all$RoofMatl)
all$LandContour <- as.factor(all$LandContour)
all$BldgType <- as.factor(all$BldgType)
all$HouseStyle <- as.factor(all$HouseStyle)
all$Condition1 <- as.factor(all$Condition1)
all$Condition2 <- as.factor(all$Condition2)
all$GarageArea <- as.numeric(all$GarageArea)

na_count_test <-sapply(all, function(y) sum(length(which(is.na(y)))))
na_count_test <- data.frame(na_count_test)


write_excel_csv(all, "New_Data.csv")
```

## 2. Inference 
In this section we analyze the relationship between House sales Price and distance from Utah State University.  

### 2.1 Hypothesis
We hypothesize that there is a positive association between final sales price and distance from ISU. In other words, houses that are further away from the university are, in general, sold for a higher price than those in neighborhoods closer to ISU. Without doing any data analysis, we know that there is a golf course at the opposite end of city from the University, which could be evidence supporting our hypothesis. Additionally, student housing usually has a lower price in comparison with other non-student houses, if this is true in Ames, then our hypothesis should be correct.

<center>
  __Hypothesis:__ 
</center>
  
$$Sales Price = \beta_0 + \beta_{Dist. ISU} + \beta_{House Quality} + \beta_{Size} + ... + \mu$$
  
```{r, message=FALSE, warning=FALSE}
#Load treated and processed data
New_Data <- read_csv("/Volumes/KINGSTON/MDP_2018/Applied Data Science/Final Project/Final-Project---Applied-Data-Science-master/Dataset/Processed/New_Data.csv")

#Load the data with distance between ISU and

distance <- read_csv("/Volumes/KINGSTON/MDP_2018/Applied Data Science/Final Project/Final-Project---Applied-Data-Science-master/Dataset/Processed/distance.csv")

#Add the distance data to both Train and Test datasets.
d_train <- left_join(New_Data, distance, by = "Neighborhood")
```

```{r message=FALSE, warning=FALSE}
#d_train1 <- d_train[,-c(73, 75, 7, 74, 57,4, 13, 83,84)]

d_train_noNAs <- na.omit(d_train)

df <- data.frame(tapply(d_train_noNAs$SalePrice, d_train_noNAs$Neighborhood, mean))

library(data.table)
library(forcats)

neighb <- setDT(df, keep.rownames = TRUE)[]
colnames(neighb)[2] <- "mean.price"

#Order values in ascending order.
neighb <- neighb[order(mean.price), ]
neighb$rn <- factor(neighb$rn, levels=unique(as.character(neighb$rn)) )


  
#Create categories for mean sales price of the neighborhood.
  
d_train_noNAs$Mean.price <- cut(d_train_noNAs$SalePrice, breaks = c(100,200000,300000, Inf), labels = c("Lower", "Median ", "Higher") )

```

##Ames, Iowa

Ames, IA located in the midwest of the United States is predominantly a college city. Ames has a population of 62,815 people with a median age of 23.2. The city is home for the Iowa State University (ISU) and its 35,000 students (excluding postdoc students and faculty members). Not surprisingly, the city has a median household income of $41,616. 

According to Data USA, the median property value in Ames, IA is $174,300, and the homeownership rate is 41%. The average commute time is 15.7 minutes. The average car ownership in Ames, IA is 2 cars per household. [^A]

[^A]: https://datausa.io/profile/geo/ames-ia/ 


From the data we can see that three neighborhoods - NoRidge, NridgHt, and StoneBr - have the highest average sales price, \$300,000 or more. We classified these three neighborhoods as "High", in regards to their average sales price. Similarly, we identified two other stratas of neighborhoods one as "Medium" with the average sales price between $150,000 and \$300,000, and "Low" all neighborhoods with average sales price up to \$150,000.

  
```{r}
#plot the results
ggplot(neighb) + geom_col(aes(neighb$rn, y = neighb$mean.price,  fill = df$rn)) + 
  coord_flip() + 
  ggtitle("Average Sales Price by Neighborhood") +
  ylab("Average Sales Price") + scale_y_continuous(labels = scales::dollar) +
  xlab("Neighborhood") + coord_flip()
```


Table XX summarizes the number of houses and their average price per neighborhood.

<center>

`Table XX`

```{r}
#Distribution of houses per neighborhood category
pander(table(d_train_noNAs$Neighborhood, d_train_noNAs$Mean.price))
```

</center>

##Hypothesis
  
  The city of Ames is a college city, with approximately 35,993 students (exluding postdocs), accouting for more than 57% of the population. This is a conservative estimate since it does not include the university's staff and contractors. Arguebly, at least 60% of Ames works and/or studies at the Iowa State University.

`Then, what is the relationship between distance from the university and the final sales price of a ho
  Our dataset does not have the exact location of each house, so we used the average distance between the approximate center point of each neighborhood to the center of IUS campus. We used google maps to calculate the distance in miles by car and walking, as well as, the time it would take to go from the location to the IUS campus. Since the campus is fairly large we used Global Café, 513 Farm House Ln, Ames, IA, 50010, as our reference point representing the University.


`Table XX: New variables`
  
| Variable      | Description                                                                                  |
|---------------|----------------------------------------------------------------------------------------------|
| Neighborhood  | The name of the neighborhood from the original data.                                         |
| FullName      | Full name of the neighborhood.                                                               |
| Lat/Log       | The approximate latitude and longitude of the reference point for each neighborhood.         |
| Car.miles     | Average distance of suggested routes between the reference point and the ISU campus.         |
| Car.Time.min  | Average journey duration of suggested routes between the reference point and the ISU campus. |
| Walk.miles    | Average distance of suggested routes between the reference point and the ISU campus.         |
| Walk.time.min | Average journey duration of suggested routes between the reference point and the ISU campus. |
| Bus.time.min  | Average journey duration of suggested routes between the reference point and the ISU campus. |

### 2.2 Understanding the important variables

#### SalePrice
```{r}
summary(all$SalePrice)
hist(all$SalePrice, breaks=seq(30000, 760000, 500),prob=TRUE )
lines(density(all$SalePrice), col="blue", lwd=2)       
lines(density(all$SalePrice, adjust=2), lty="dotted")
```

#### Overall Quality 
  
OverallQual: Rates the overall material and finish of the house

       10	Very Excellent
       9	Excellent
       8	Very Good
       7	Good
       6	Above Average
       5	Average
       4	Below Average
       3	Fair
       2	Poor
       1	Very Poor
       
Overall Quality is observed to have positive correlation with SalePrice. It does not seem to have noticeble outliers apart from a point in level 4. 
```{r}
ggplot(data=all[!is.na(all$SalePrice),], aes(x=factor(OverallQual), y=SalePrice))+
        geom_boxplot() + labs(x='Overall Quality') +
        scale_y_continuous(breaks= seq(0, 800000, by=100000), labels = comma)
```

#### 'GrLivArea' (‘Above Grade’ Living Area)
GrLivArea: Above grade (ground) living area square feet  
GrLivArea is observed to have positive correlation with SalePrice, which make sense since big houses are generally more expensive than smaller. One thing worth noticing is that there are two houses with really large living areas and low SalePrice (potential to be outliers). I checked the data and found out they are houses 524, 1299 and 2550(NA for SalePrice).
```{r}
ggplot(data=all[!is.na(all$SalePrice),], aes(x=GrLivArea, y=SalePrice))+
        geom_point(col='black') + geom_smooth(method = "lm", se=FALSE, color="blue", aes(group=1)) +
        scale_y_continuous(breaks= seq(0, 800000, by=100000), labels = comma)
outlier1 <- all%>% filter(GrLivArea > 4500)
outlier1$Id
all[c(524, 1299, 2550), c('SalePrice', 'GrLivArea', 'OverallQual')]

```

#### GarageCars
  
The Size of garage shows a positive correlation with SalePrice from level 0-3 but fall at level 4 (non-linear correlation). It make sense because only big houses have large garage, and bigger house usually have higher SalePrice. Also for houses with a garage of capacity of 3 cars(level 3), there are several outliners with much higher SalePrice existing, it must be other factors affecting the outcome price. Something might happen for houses with level 4 garage that needed to explained.
```{r, GarageCars}

summary(all$GarageCars)
ggplot(data=all[!is.na(all$SalePrice),], aes(x=factor(GarageCars), y=SalePrice))+
        geom_boxplot() + labs(x='Size of Garage/car') +
        scale_y_continuous(breaks= seq(0, 800000, by=100000), labels = comma)

outlier2 <- all %>% 
  filter(GarageCars == 3)%>%
  filter(SalePrice > 700000)

outlier2$Id
all[c(524, 1299, 692, 1183), c('SalePrice', 'GrLivArea', 'OverallQual','GarageCars')]

```

#### BsmtFinSF1
```{r, BsmtFinSF1}
ggplot(data=all[!is.na(all$SalePrice),], aes(x=BsmtFinSF1, y=SalePrice))+
        geom_point(col='black') + geom_smooth(method = "lm", se=FALSE, color="blue", aes(group=1))+
        scale_x_continuous(limits = c(100 ,2500))

```

#### Neighborhood

Neighborhood: Physical locations within Ames city limits

       Blmngtn	Bloomington Heights
       Blueste	Bluestem
       BrDale	Briardale
       BrkSide	Brookside
       ClearCr	Clear Creek
       CollgCr	College Creek
       Crawfor	Crawford
       Edwards	Edwards
       Gilbert	Gilbert
       IDOTRR	Iowa DOT and Rail Road
       MeadowV	Meadow Village
       Mitchel	Mitchell
       Names	North Ames
       NoRidge	Northridge
       NPkVill	Northpark Villa
       NridgHt	Northridge Heights
       NWAmes	Northwest Ames
       OldTown	Old Town
       SWISU	South & West of Iowa State University
       Sawyer	Sawyer
       SawyerW	Sawyer West
       Somerst	Somerset
       StoneBr	Stone Brook
       Timber	Timberland
       Veenker	Veenker
       
```{r neighborhood}

df <- data.frame(tapply(train$SalePrice, train$Neighborhood, mean))
library(data.table)
setDT(df, keep.rownames = TRUE)[]
colnames(df) <- c('Neighborhood','Average SalePrice')

ggplot(df, aes(x = Neighborhood, y= `Average SalePrice`)) + 
  geom_col(aes(fill = df$Neighborhood))+
  geom_hline(yintercept=mean(train$SalePrice), linetype="dashed", color = "red")+
        theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggplot(data=train, aes(x=(Neighborhood))) +
        geom_histogram(stat='count')+
        geom_label(stat = "count", aes(label = ..count.., y = ..count..), size=3)+
        theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

#### YearBuilt
```{r, yearbuilt}

ggplot(data=all[!is.na(all$SalePrice),], aes(x=YearBuilt, y=SalePrice))+
        geom_point(col='black') + geom_smooth(method = "lm", se=FALSE, color="blue", aes(group=1))+
        scale_x_continuous(limits = c(1860 ,2020))
```


### 2.3 Inferential Models
We will investigate the validity of our hypothesis through an Ordinary Least Square model.

##Ordinary Least Squares

First we ran a naive OLS model including only the variables of interest in the model. The model had statistical significance with an F-statistic of 26.33 and R^{2} of 0.1097. Our variables of interest were statistically significant at the 0.05 level with the exception of Car.Miles. In model 2 we included the "Overall Quality" variable and our results improved significantly resulting in a R^{2} of 0.6537 and an F-statistic of 335.7 . Lastly, Model 3 included 86 all variables in the model,the mechanics behind the OLS regression automatically leads to a higher R^{2} (0.94) but the overall strength of the model was not acceptable (F-statistic of 40.4). Table X compares the coefficients to all the three models.

#### Model 1.0
<center>
__Model 1.0: Car.Time.min__
</center>
```{r Naive Model, message=FALSE, warning=FALSE}
#OLS Model 1 - very naive considering only the variab
# naive.model <- lm(SalePrice ~ Car.miles + OverallQual + OverallCond + Mean.price  , data = d_train_noNAs)
# summary(naive.model)

print("Model 1.0: Car.Time.min")

naive.model0 <- lm(SalePrice ~ Car.Time.min + OverallQual +OverallCond + Mean.price  , data = d_train_noNAs)
summary(naive.model0)

```

#### Model 1.1
<center>
__Model 1.1: Walk.Time.min__
</center>
```{r}
# naive.model1 <- lm(SalePrice ~ Walk.miles + OverallQual +OverallCond + Mean.price  , data = d_train_noNAs)
# summary(naive.model1)

naive.model1.1 <- lm(SalePrice ~ Walk.time.min + OverallQual +OverallCond + Mean.price  , data = d_train_noNAs)
summary(naive.model1.1)
```

#### Model 1.2

<center>
__Model 1.2: Bus.Time.min__
</center>
```{r}
naive.model2 <- lm(SalePrice ~ Bus.time.min + OverallQual +OverallCond + Mean.price  , data = d_train_noNAs)
summary(naive.model2)
```

All our three naive models did not have good results, but when comparing the three models, Model 1.2 performed relatively better. We will use Model 1.2. in a more comprehensive model
with all the variables in our dataset.

<center>

`Table XX`

| Variable      | Model 1.0 - Car.Time.min | Model 1.1 - Walk.time.min | Model 1.2 - Bus.time.min |
|---------------|--------------------------|---------------------------|--------------------------|
| Estimate      | -12.27                   | 62.27                     | 151.3                    |
| Std. Error    | 415.02                   | 55.19                     | 117.8                    |
| t value       | -0.03                    | 1.128                     | 1.284                    |
| Pr(>abs(t))   | 0.9764                   | 0.2594                    | 0.1993                   |
| Adj R Squared | 0.8387                   | 0.8389                    | 0.839                    |
| F-Statistic   | 1117                     | 1119                      | 1119                     |
</center>

#### Model 2.0
<center>
__Model 2.0: Comprehensive model + Bus.Time.min__
</center>
```{r}
m2.train <- lm(SalePrice ~ Bus.time.min + ., data = d_train_noNAs)

#Show only relevant coefficients
a2Pval <- summary(m2.train)$coefficients[1:2, 1:4]

a2Pval
```
Although we added all variables to Model 2.0, we also loose degrees of freedom and the entire model becomes relatively weak with an F-Statistic of 52.94. Table XX summarizes the results in regards to our variable of interest. Regardless, our variable of interest was not statistically significant nor substantive. However, the model can be improved by including only relevant variables and our variable of interest. We will use Lasso to sort through the 87 variables in our model and include only the best variables into a new model.

<center>

`Table XX`

| Variable      | Model 2.0 - Full model + Bus.time.min |
|---------------|---------------------------------------|
| Estimate      | 269.9592  |
| Std. Error    | 547.9578  |
| t value       | 0.4926642 |
| Pr(>abs(t)      | 0.6223962 |
| Adj R Squared | 0.9417    |
| F-Statistic   | 52.94     |

  
</center>

##Lasso

The Lasso function will help us select the variables with the highest predictive power on Sales Price. It might also give us a clearer picture that will help us better assess our hypothesis.

Each curve corresponds to a variable. It shows the path of its coefficient against the ℓ1-norm of the whole coefficient vector at as λ varies. The axis above indicates the number of nonzero coefficients at the current λ, which is the effective degrees of freedom (df) for the lasso. Users may also wish to annotate the curves; this can be done by setting label = TRUE in the plot command.


```{r}
set.seed(12345) # = Seed for replication

#d_train_noNAs <- na.omit(d_train)

x <- model.matrix(SalePrice ~ ., data = d_train_noNAs)[ ,-79]
y <- d_train_noNAs$SalePrice
 
### We then fit a Lasso regression model (alpha = 1)
fit.lasso <- glmnet(x, y, alpha = 1, family = "gaussian")
plot(fit.lasso, xvar = "lambda", label = TRUE)
 
### Now we cross-validate
cv.lasso <- cv.glmnet(x, y)

#cv.lasso$lambda.min 
#cv.lasso$lambda.1se

#coef(cv.lasso, s = "lambda.min")
```

Cross-Validation plot suggests that the model works best when it has approximately 11 predictors (when using lambda for the most regularized model such that error is within one standard error of the minimum.). Lasso helped to reduce the number of variables by over 87%. We can use cross-validation to extract coefficients that collectively minimize mean squared error.
 
It includes the cross-validation curve (red dotted line), and upper and lower standard deviation curves along the λ sequence (error bars). Two selected λ’s are indicated by the vertical dotted lines (see below). 

```{r}
plot(cv.lasso)
```
 
```{r, warning=FALSE, message=FALSE, echo=FALSE}
set.seed(12345) # = Seed for replication

### Extract coefficients corresponding to lambda.min (minimum mean cross-validated error)
myCoefs <- coef(cv.lasso, s = "lambda.1se")

# print(as.matrix(min.coef))

myLasso.Results <- data.frame(
  features = myCoefs@Dimnames[[1]][ which(myCoefs != 0 ) ], #intercept included
  coefs    = myCoefs              [ which(myCoefs != 0 ) ]  #intercept included
)

# myLasso.Results$level.coefs <- cut(myLasso.Results$coefs, 5, labels = c("lowest","low", "med", "high", "highest"))
# 
# table(myLasso.Results$level.coefs)
# 
# summary(myLasso.Results$coefs)
# 
# ggplot(myLasso.Results) + geom_bar(aes(myLasso.Results$level.coefs, fill = myLasso.Results$level.coefs)) + coord_flip()

```


<center>

`Table XX: Lasso coefficients`

```{r}
pander(myLasso.Results)
#pander(filter(myLasso.Results, level.coefs != "low" ))
```

</center>

### Model 3.0 - Improved OLS

Our new model with only 12 variables (instead of all variables) has an adjusted R-squared of __0.8892__ and an F-Statistic of __616.1__ - a significant result in comparison to an adjusted R-square of __0.9417__ and an F-statistic of 52.94 (including all 80 variables!). In this adapted model, our variables of interest (Bus.time.min) have a very small coefficient and all of them (except for Car.miles) are statistically significant at the 0.05 level).


```{r}
myLasso.Results <- myLasso.Results[-1,]
var.lasso <- myLasso.Results[,1]
var.lasso

m4.train <- lm(SalePrice ~ Bus.time.min + MSZoning + OverallQual + YearBuilt +  TotalBsmtSF + X1stFlrSF + GrLivArea + Fireplaces + GarageCars + Mean.price
, data = d_train_noNAs)

summary(m4.train)
```

##Results

The result from Model 3 provides evidece against the initial hypothesis that there is a positive association between final sales price and distance from ISU. In fact, the model reveals the opposite, on average, for each additional mintute it takes from a neighborhood to ISU campus is associated with a price drop of approximately $203.60 in the final sales price. Although the result is statistically significant at the 0.05 level, the result was not substantive. Therefore, we can conclude that distance from the ISU campus is not an important factor when determining the house/apt sales price.

 </div>
</div>


## 3 Predictive Model
In this section, we aim to find the most important varibals that affect the Sales Price and build predictive models based on that. We will compare the effectiveness of different predictive models and choose the best model.

### 3.1 Find Important variables
#### Random Forest 
```{r}
# Neighborhood
train <- read.csv("/Users/lizhizicui/Desktop/Final-Project---Applied-Data-Science/Src/New_Data.csv")

train <- na.omit(train)
train$GarageYrBlt <- as.integer(train$GarageYrBlt)
```


### 3.1 Correlation with SalePrice
We would like to see What numeric variables correlated with SalePrice.

According to the correlation table, variable 'Overall Quality'(Overall material and finish quality), the 'GrLivArea' (‘Above Grade’ Living Area) and 'GarageCars'(Size of garage in car capacity) are the top 3 variables that highly correlated to SalePrice.
For the rest,it is also clear the multicollinearity is an issue. For example: the correlation between GarageCars and GarageArea is very high (0.89), and both have similar (high) correlations with SalePrice. The other 6 six variables with a correlation higher than 0.5 with SalePrice are: -TotalBsmtSF: Total square feet of basement area -1stFlrSF: First Floor square feet -FullBath: Full bathrooms above grade -TotRmsAbvGrd: Total rooms above grade (does not include bathrooms) -YearBuilt: Original construction date -YearRemodAdd: Remodel date (same as construction date if no remodeling or additions)

```{r correlation2}
all_numVar <- train[, numericVars]
cor_numVar <- cor(all_numVar, use="pairwise.complete.obs")
cor_sorted <- as.matrix(sort(cor_numVar[,'SalePrice'], decreasing = TRUE))
 #select only high corelations
CorHigh <- names(which(apply(cor_sorted, 1, function(x) abs(x)>0.5)))
cor_numVar <- cor_numVar[CorHigh, CorHigh]

corrplot.mixed(cor_numVar, tl.col="black", tl.pos = "lt", tl.cex = 0.7,cl.cex = .7, number.cex=.7)
```
Since there are several paris of varibles are highly correlated to each other, we can simply drop one of them.
Therefore, I chose to drop 'TotalRmsAbvGrd'(vs GrLivArea); 'GarageArea'(vs GarageCars); 'TotalBsmtSF' (vs X1stFkrSF); 'YearRemodAdd'(vs YearBiult)

```{r drop variables and outliers}
dropVars <- c('YearRemodAdd', 'GarageArea', 'TotalBsmtSF', 'TotalRmsAbvGrd')

train <- train[,!(names(train) %in% dropVars)]

train <- train[-c(524, 1299, 2550),]
write_csv(train, 'Final_data.csv')
```

### 3.2 Finding important variables with Random Forest
The correlation table above provide a good overview of the most important numeric variables and multicolinerity among those variables. Next, I want to get an overview of the most important variables including the categorical variables.
```{r rf again}
train <- read.csv('Final_data.csv')
numericVars <- which(sapply(train, is.numeric)) #index vector numeric variables
factorVars <- which(sapply(train, is.factor)) #index vector factor variables
cat('There are', length(numericVars), 'numeric variables, and', length(factorVars), 'categoric variables')

set.seed(123)
quick_RF <- randomForest(x=train[, -76], y=train$SalePrice, ntree=200,importance=TRUE)
imp_RF <- importance(quick_RF)
imp_DF <- data.frame(Variables = row.names(imp_RF), MSE = imp_RF[,1])
imp_DF <- imp_DF[order(imp_DF$MSE, decreasing = TRUE),]

ggplot(imp_DF[1:20,], aes(x=reorder(Variables, MSE), y=MSE, fill=MSE)) + geom_bar(stat = 'identity') + labs(x = 'Variables', y= '% increase MSE if variable is randomly permuted') + coord_flip() + theme(legend.position="none")
```

### 3.3 Models
We pick variables that increase MSE if variable is randomly permuted greater than 10% (exclude two variables which are X1ndFlrSF and X2ndFlrSF which representing First/Second floor square feet, I think the information is included in GrLivArea) and run a ols model. The ols model has an (adjusted) R2 that is greater than 0.8
```{r ols}

train$Mean.price <- cut(train$SalePrice, breaks = c(100,200000,300000, Inf), labels = c("Lower", "Median ", "Higher") )
a <- train%>%
  filter(Mean.price == 'Higher')

train1 <- train[1:1092,]
test1 <- train[1092:1447,]

ols <- lm(Mean.price ~ GarageCars+ BsmtFinSF1+ OverallQual+ Neighborhood + GrLivArea + OverallQual*YearBuilt ,data = train1)
summary(ols)$adj.r.squared

y_hat_ols <- predict(ols, newdata = test1)
y_hat_ols
summary(y_hat_ols)
z_ols <- as.integer(y_hat_ols > 1)
z_ols
table(test1$Mean.price, z_ols)
```
But I don't want to just make my conclusion base on the basis of (possibly flawed) intuition about how real estate prices work.
Thus, I start with a model that includes all available predictors, an interaction between OverallQual and YearBuilt (because the quality of Older houses may not as good as the new ones).
```{r step function}
 # step function 

train1 <- subset(train1, select = -c(Utilities,Street))
test1 <- subset(test1, select = -c(Utilities,Street))
test1 <- test1[!test1$Id == 1004,]
test1 <- test1[!test1$Id == 1299,]
test1 <- test1[!test1$Id == 1001,]
test1 <- test1[!test1$Id == 1188,]
test1 <- test1[!test1$Id == 411,]
test1 <- test1[!test1$Id == 251,]
test1 <- test1[!test1$Id == 596,]
```

### Fitting Vector Generalized Linear Models
```{r, Vector Generalized Linear Models}
library(VGAM)
# with interaction term
fit_log <- vglm(Mean.price ~ GarageCars+ BsmtFinSF1+ OverallQual+ Neighborhood + GrLivArea, family=multinomial, data=train1)
# summarize the fit
summary(fit_log)

# make predictions
probabilities <- predict(fit_log, newdata = test1, type="response")
predictions <- apply(probabilities, 1, which.max)
predictions[which(predictions=="Lower")] <- levels(test1$Mean.price)[1]
predictions[which(predictions=="Median")] <- levels(test1$Mean.price)[2]
predictions[which(predictions=="Higher")] <- levels(test1$Mean.price)[3]

# summarize accuracy
tbl_log <- table(predictions, test1$Mean.price)
tbl_log
sum(diag(tbl_log))/sum(tbl_log)
```

### Fitting Linear Discriminant Analysis Models
```{r lda}
library(MASS)
LDA <- lda(Mean.price ~ GarageCars+ BsmtFinSF1+ OverallQual+ Neighborhood + GrLivArea,data = train1)

y_hat_LDA <- predict(LDA, newdata = test1)
summary(y_hat_LDA$posterior)
z_LDA <- y_hat_LDA$class
tbl_LDA <- table(test1$Mean.price, z_LDA)
tbl_LDA
sum(diag(tbl_LDA))/sum(tbl_LDA)
```

### Fitting Support Vector Machine Models
```{r svm}
library(e1071)

classifier = svm(formula = Mean.price ~ . ,data = train1[,-74] ,
                 type = 'C-classification',
                 kernel = 'linear')
help("svm")
# Predicting the Test set results
y_pred = predict(classifier, newdata = test1[,-c(74,75)])

# Making the Confusion Matrix
cm = table(test1[,75], y_pred)
cm
sum(diag(cm))/sum(cm)
```

### Fitting Random Forest Model
```{r rf predict}

RF <- randomForest(x=train1[, (names(train1) %in% c("OverallQual","BsmtFinSF1","GarageCars",'Neighborhood','GrLivArea'))], y=train1$Mean.price, ntree=150,importance=TRUE)
y_pred = predict(RF, newdata = test1[-c(74,75)])

# Making the Confusion Matrix
rf_cm = table(test1[,75], y_pred)
rf_cm
sum(diag(rf_cm))/sum(rf_cm)
plot(RF)

```


### 3.4 Results

## 4. Conclusion


Next Steps:
1)Time Series Analysis on Pricing
2)Cluster Analysis on Neighborhood
